{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd15d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "U-CNN (Conv) Pipeline \n",
    "=================================================\n",
    "Goal\n",
    "----\n",
    "1) Train ONLY the U-CNN pipeline end-to-end on your Mac (M4) using PyTorch MPS:\n",
    "\n",
    "2) Supervised classifier training with heavy imbalance handling, F1-based threshold tuning,\n",
    "   early stopping, checkpoints, resume-on-start.\n",
    "\n",
    "3) Produce submission CSV: columns [label, ID] where ID = \"test/xxxxx.jpg\".\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os, json, time, math, random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "# For safety with partially-corrupt JPEGs\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Repro + device\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_device()\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Paths\n",
    "# -----------------------------\n",
    "DATA_ROOT = Path(\"/Users/zamir/AI/dsm/dsm-2025\")\n",
    "TRAIN_DIR = DATA_ROOT / \"train\"\n",
    "NEG_DIR   = TRAIN_DIR / \"0\"\n",
    "POS_DIR   = TRAIN_DIR / \"1\"\n",
    "UNLAB_DIR = TRAIN_DIR / \"Unlabeled\"\n",
    "TEST_DIR  = DATA_ROOT / \"test\"\n",
    "\n",
    "OUT_DIR   = DATA_ROOT / \"outputs_ucnn\"\n",
    "CKPT_DIR  = OUT_DIR / \"_checkpoints\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"TRAIN_DIR:\", TRAIN_DIR)\n",
    "print(\"NEG_DIR  :\", NEG_DIR)\n",
    "print(\"POS_DIR  :\", POS_DIR)\n",
    "print(\"UNLAB_DIR:\", UNLAB_DIR)\n",
    "print(\"TEST_DIR :\", TEST_DIR)\n",
    "print(\"OUT_DIR  :\", OUT_DIR)\n",
    "print(\"CKPT_DIR :\", CKPT_DIR)\n",
    "\n",
    "assert NEG_DIR.exists(), f\"Missing: {NEG_DIR}\"\n",
    "assert POS_DIR.exists(), f\"Missing: {POS_DIR}\"\n",
    "assert TEST_DIR.exists(), f\"Missing: {TEST_DIR}\"\n",
    "assert UNLAB_DIR.exists(), f\"Missing: {UNLAB_DIR}\"\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Speed knobs (Mac MPS)\n",
    "# -----------------------------\n",
    "# Mac MPS likes smallish batches; keep model lightweight.\n",
    "IMG_SIZE = 224\n",
    "NUM_WORKERS = 0  # IMPORTANT on macOS notebooks to avoid multiprocessing pickling issues\n",
    "PIN_MEMORY = False  # MPS doesn't benefit like CUDA\n",
    "PERSISTENT_WORKERS = False\n",
    "\n",
    "# To fit memory and time budget:\n",
    "BATCH_SSL = 32\n",
    "BATCH_CLS = 32\n",
    "\n",
    "# Time budget controls\n",
    "MAX_HOURS_BUDGET = 4.0\n",
    "START_TIME = time.time()\n",
    "\n",
    "# SSL subset \n",
    "SSL_SUBSET = 30000  \n",
    "EPOCHS_SSL = 10     \n",
    "EPOCHS_CLS = 25    \n",
    "EARLY_STOP_PATIENCE = 10\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Helpers\n",
    "# -----------------------------\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "\n",
    "def now_h():\n",
    "    return (time.time() - START_TIME) / 3600.0\n",
    "\n",
    "def budget_ok():\n",
    "    return now_h() <= MAX_HOURS_BUDGET\n",
    "\n",
    "def list_images(folder: Path) -> List[Path]:\n",
    "    # Uses full paths; sorted for determinism\n",
    "    paths = [p for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in IMG_EXTS]\n",
    "    paths.sort()\n",
    "    return paths\n",
    "\n",
    "def safe_open_rgb(path: Path) -> Image.Image:\n",
    "    # Robust open; if fails, raise to be filtered earlier.\n",
    "    img = Image.open(path)\n",
    "    img = img.convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "def validate_images(paths: List[Path], max_check: int = 2000) -> Tuple[List[Path], List[Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Quick validation on up to max_check images (speed).\n",
    "    Returns: good_paths, bad_list [(path, err)]\n",
    "    \"\"\"\n",
    "    bad = []\n",
    "    good = []\n",
    "    n = len(paths)\n",
    "    # Only sample for speed\n",
    "    idxs = list(range(n))\n",
    "    random.shuffle(idxs)\n",
    "    idxs = idxs[: min(max_check, n)]\n",
    "    bad_set = set()\n",
    "\n",
    "    for i, idx in enumerate(idxs, 1):\n",
    "        p = paths[idx]\n",
    "        try:\n",
    "            _ = safe_open_rgb(p)\n",
    "        except Exception as e:\n",
    "            bad.append((str(p), repr(e)))\n",
    "            bad_set.add(p)\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(f\"[validate] checked {i}/{len(idxs)} bad={len(bad)}\")\n",
    "\n",
    "    for p in paths:\n",
    "        if p not in bad_set:\n",
    "            good.append(p)\n",
    "    return good, bad\n",
    "\n",
    "def save_json(path: Path, obj: dict):\n",
    "    path.write_text(json.dumps(obj, indent=2))\n",
    "\n",
    "def load_json(path: Path) -> dict:\n",
    "    return json.loads(path.read_text())\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Build file lists (FULL PATHS ONLY)\n",
    "# -----------------------------\n",
    "neg_paths = list_images(NEG_DIR)\n",
    "pos_paths = list_images(POS_DIR)\n",
    "unlab_paths = list_images(UNLAB_DIR)\n",
    "test_paths = list_images(TEST_DIR)\n",
    "\n",
    "print(\"\\nDisk counts (by full path):\")\n",
    "print(\"train/0:\", len(neg_paths))\n",
    "print(\"train/1:\", len(pos_paths))\n",
    "print(\"unlabeled:\", len(unlab_paths))\n",
    "print(\"test:\", len(test_paths))\n",
    "\n",
    "neg_names = set(p.name for p in neg_paths)\n",
    "pos_names = set(p.name for p in pos_paths)\n",
    "overlap = sorted(list(neg_names & pos_names))\n",
    "print(\"\\nFilename overlap (should be >0 in your dataset):\", len(overlap))\n",
    "if len(overlap) > 0:\n",
    "    print(\"Overlap examples:\", overlap[:10])\n",
    "print(\"‚úÖ This code is safe because it never keys by filename-only.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Build supervised split (full paths + labels)\n",
    "# -----------------------------\n",
    "# Ensures class ratio doesn't collapse:\n",
    "# - Use ALL positives (293)\n",
    "# - Sample negatives to keep training stable (e.g., 10x negatives)\n",
    "# - Keep val with some positives\n",
    "\n",
    "POS_ALL = pos_paths[:]  # 293\n",
    "NEG_ALL = neg_paths[:]  # 5000\n",
    "\n",
    "# Choose negative multiplier\n",
    "NEG_MULT = 10  # \"extreme\" push: still keeps majority but reduces dominance\n",
    "neg_needed = min(len(NEG_ALL), len(POS_ALL) * NEG_MULT)\n",
    "random.shuffle(NEG_ALL)\n",
    "NEG_USE = NEG_ALL[:neg_needed]\n",
    "\n",
    "# Merge and stratify split\n",
    "all_labeled = [(p, 1) for p in POS_ALL] + [(p, 0) for p in NEG_USE]\n",
    "random.shuffle(all_labeled)\n",
    "\n",
    "# Stratified split\n",
    "val_frac = 0.25\n",
    "pos_items = [(p,y) for (p,y) in all_labeled if y==1]\n",
    "neg_items = [(p,y) for (p,y) in all_labeled if y==0]\n",
    "\n",
    "n_pos_val = max(1, int(len(pos_items)*val_frac))\n",
    "n_neg_val = max(1, int(len(neg_items)*val_frac))\n",
    "\n",
    "pos_val = pos_items[:n_pos_val]\n",
    "neg_val = neg_items[:n_neg_val]\n",
    "pos_trn = pos_items[n_pos_val:]\n",
    "neg_trn = neg_items[n_neg_val:]\n",
    "\n",
    "train_items = pos_trn + neg_trn\n",
    "val_items   = pos_val + neg_val\n",
    "random.shuffle(train_items)\n",
    "random.shuffle(val_items)\n",
    "\n",
    "print(\"\\nSplit:\")\n",
    "print(f\"train: {len(train_items)}  pos={sum(y for _,y in train_items)} neg={sum(1-y for _,y in train_items)}\")\n",
    "print(f\"val  : {len(val_items)}  pos={sum(y for _,y in val_items)} neg={sum(1-y for _,y in val_items)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Transforms (fixed 224x224 ALWAYS)\n",
    "# -----------------------------\n",
    "# Train aug: moderate (too strong can hurt small positives)\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.2),\n",
    "    transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.10, hue=0.02),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "eval_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# SSL transform: produce clean + corrupted version\n",
    "ssl_clean_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def corrupt_tensor(x: torch.Tensor) -> torch.Tensor:\n",
    "    # Simple corruption: gaussian noise + random masking blocks\n",
    "    # (Keeps it fast; no fancy ops)\n",
    "    noise = torch.randn_like(x) * 0.08\n",
    "    x2 = (x + noise).clamp(0, 1)\n",
    "\n",
    "    # random cutout blocks\n",
    "    _, H, W = x2.shape\n",
    "    for _ in range(6):\n",
    "        h = random.randint(12, 40)\n",
    "        w = random.randint(12, 40)\n",
    "        y0 = random.randint(0, H - h)\n",
    "        x0 = random.randint(0, W - w)\n",
    "        x2[:, y0:y0+h, x0:x0+w] = 0.0\n",
    "    return x2\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Datasets (FULL PATH IDs)\n",
    "# -----------------------------\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, items: List[Tuple[Path,int]], tfm):\n",
    "        self.items = items\n",
    "        self.tfm = tfm\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p, y = self.items[idx]\n",
    "        img = safe_open_rgb(p)\n",
    "        x = self.tfm(img)\n",
    "        # return FULL PATH as id to avoid collisions\n",
    "        return x, torch.tensor(y, dtype=torch.long), str(p)\n",
    "\n",
    "class UnlabeledDenoiseDataset(Dataset):\n",
    "    def __init__(self, paths: List[Path]):\n",
    "        self.paths = paths\n",
    "\n",
    "    def __len__(self): return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.paths[idx]\n",
    "        img = safe_open_rgb(p)\n",
    "        clean = ssl_clean_tf(img)\n",
    "        corrupt = corrupt_tensor(clean)\n",
    "        return corrupt, clean, str(p)\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, paths: List[Path]):\n",
    "        self.paths = paths\n",
    "\n",
    "    def __len__(self): return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.paths[idx]\n",
    "        img = safe_open_rgb(p)\n",
    "        x = eval_tf(img)\n",
    "        return x, str(p)\n",
    "\n",
    "def make_loader(ds, batch, shuffle):\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        persistent_workers=PERSISTENT_WORKERS\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Model: Tiny U-Net style encoder + classifier head\n",
    "# -----------------------------\n",
    "def conv_block(in_ch, out_ch):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "        nn.GroupNorm(8, out_ch),\n",
    "        nn.SiLU(inplace=True),\n",
    "        nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "        nn.GroupNorm(8, out_ch),\n",
    "        nn.SiLU(inplace=True),\n",
    "    )\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, base=32):\n",
    "        super().__init__()\n",
    "        self.stem = conv_block(3, base)\n",
    "        self.down1 = nn.Sequential(nn.MaxPool2d(2), conv_block(base, base*2))\n",
    "        self.down2 = nn.Sequential(nn.MaxPool2d(2), conv_block(base*2, base*4))\n",
    "        self.down3 = nn.Sequential(nn.MaxPool2d(2), conv_block(base*4, base*8))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.stem(x)      # 224\n",
    "        x1 = self.down1(x0)    # 112\n",
    "        x2 = self.down2(x1)    # 56\n",
    "        x3 = self.down3(x2)    # 28\n",
    "        return x3  # latent\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, base=32):\n",
    "        super().__init__()\n",
    "        self.up2 = nn.Sequential(nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False),\n",
    "                                 conv_block(base*8, base*4))\n",
    "        self.up1 = nn.Sequential(nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False),\n",
    "                                 conv_block(base*4, base*2))\n",
    "        self.up0 = nn.Sequential(nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False),\n",
    "                                 conv_block(base*2, base))\n",
    "        self.out = nn.Conv2d(base, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.up2(z)   # 56\n",
    "        x = self.up1(x)   # 112\n",
    "        x = self.up0(x)   # 224\n",
    "        x = self.out(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class DenoiseAE(nn.Module):\n",
    "    def __init__(self, base=32):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(base=base)\n",
    "        self.dec = Decoder(base=base)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        return self.dec(z)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, base=32):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(base=base)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(base*8, base*4),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(base*4, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        z = self.pool(z)\n",
    "        logits = self.head(z).squeeze(1)\n",
    "        return logits\n",
    "\n",
    "# -----------------------------\n",
    "# 9) Checkpoint / resume utils\n",
    "# -----------------------------\n",
    "def ckpt_path(name: str) -> Path:\n",
    "    return CKPT_DIR / name\n",
    "\n",
    "def save_ckpt(name: str, payload: dict):\n",
    "    torch.save(payload, ckpt_path(name))\n",
    "\n",
    "def load_ckpt(name: str):\n",
    "    p = ckpt_path(name)\n",
    "    if p.exists():\n",
    "        return torch.load(p, map_location=\"cpu\")\n",
    "    return None\n",
    "\n",
    "def mark_done(flag: str, meta: dict):\n",
    "    p = CKPT_DIR / f\"{flag}.json\"\n",
    "    save_json(p, {\"done\": True, \"meta\": meta, \"time\": time.time()})\n",
    "\n",
    "def is_done(flag: str) -> bool:\n",
    "    return (CKPT_DIR / f\"{flag}.json\").exists()\n",
    "\n",
    "# -----------------------------\n",
    "# 10) Metrics (F1 + threshold search)\n",
    "# -----------------------------\n",
    "def f1_at_threshold(probs: np.ndarray, y_true: np.ndarray, thr: float) -> Tuple[float,float,float]:\n",
    "    y_pred = (probs >= thr).astype(np.int32)\n",
    "    tp = np.sum((y_pred==1) & (y_true==1))\n",
    "    fp = np.sum((y_pred==1) & (y_true==0))\n",
    "    fn = np.sum((y_pred==0) & (y_true==1))\n",
    "    prec = tp / (tp + fp + 1e-9)\n",
    "    rec  = tp / (tp + fn + 1e-9)\n",
    "    f1   = 2*prec*rec/(prec+rec+1e-9)\n",
    "    return float(f1), float(prec), float(rec)\n",
    "\n",
    "def best_threshold(probs: np.ndarray, y_true: np.ndarray) -> Tuple[float,float,float,float]:\n",
    "    best = (-1.0, 0.5, 0.0, 0.0)  # f1, thr, p, r\n",
    "    for thr in np.linspace(0.05, 0.95, 19):\n",
    "        f1, p, r = f1_at_threshold(probs, y_true, float(thr))\n",
    "        if f1 > best[0]:\n",
    "            best = (f1, float(thr), p, r)\n",
    "    return best[0], best[1], best[2], best[3]\n",
    "\n",
    "# -----------------------------\n",
    "# 11) Training loops (MPS friendly)\n",
    "# -----------------------------\n",
    "def train_ssl(ae: DenoiseAE, loader: DataLoader, epochs: int, lr: float = 2e-4):\n",
    "    ae.to(DEVICE)\n",
    "    opt = torch.optim.AdamW(ae.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    start_epoch = 0\n",
    "    resume = load_ckpt(\"ssl_last.pt\")\n",
    "    if resume is not None:\n",
    "        try:\n",
    "            ae.load_state_dict(resume[\"model\"])\n",
    "            opt.load_state_dict(resume[\"opt\"])\n",
    "            start_epoch = int(resume[\"epoch\"]) + 1\n",
    "            print(f\"üîÅ [SSL] resume from epoch {start_epoch}\")\n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è [SSL] could not load resume, starting fresh:\", e)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        if not budget_ok():\n",
    "            print(\"‚è±Ô∏è Budget exceeded, stopping SSL early.\")\n",
    "            break\n",
    "\n",
    "        ae.train()\n",
    "        t0 = time.time()\n",
    "        total = 0.0\n",
    "        n = 0\n",
    "\n",
    "        print(f\"\\nüöÄ [SSL] epoch {epoch+1}/{epochs} lr={lr:.2e}\")\n",
    "\n",
    "        for it, (x_corrupt, x_clean, _ids) in enumerate(loader, 1):\n",
    "            x_corrupt = x_corrupt.to(DEVICE)\n",
    "            x_clean   = x_clean.to(DEVICE)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            recon = ae(x_corrupt)\n",
    "            loss = F.l1_loss(recon, x_clean)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total += float(loss.item())\n",
    "            n += 1\n",
    "\n",
    "            # frequent prints (you asked)\n",
    "            if it % 50 == 0:\n",
    "                print(f\"   [SSL] it {it:5d}/{len(loader)} loss={total/n:.4f}\")\n",
    "\n",
    "        avg = total / max(n,1)\n",
    "        dt = time.time() - t0\n",
    "        print(f\"‚úÖ [SSL] epoch {epoch+1} avg_loss={avg:.4f} time={dt:.1f}s\")\n",
    "\n",
    "        save_ckpt(\"ssl_last.pt\", {\"model\": ae.state_dict(), \"opt\": opt.state_dict(), \"epoch\": epoch})\n",
    "\n",
    "    save_ckpt(\"ssl_final.pt\", {\"model\": ae.state_dict()})\n",
    "    mark_done(\"SSL_DONE\", {\"epochs\": epochs, \"subset\": len(loader.dataset)})\n",
    "    print(\"üèÅ [SSL] done\")\n",
    "\n",
    "def train_classifier(model: Classifier, train_loader: DataLoader, val_loader: DataLoader, epochs: int, lr: float = 2e-4):\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Weighted BCE (helps with imbalance)\n",
    "    # pos_weight = (#neg / #pos) in TRAIN set\n",
    "    y_train = []\n",
    "    for _, y, _ in train_loader.dataset:\n",
    "        y_train.append(int(y))\n",
    "    n_pos = sum(y_train)\n",
    "    n_neg = len(y_train) - n_pos\n",
    "    pos_weight = torch.tensor([max(1.0, n_neg / max(1, n_pos))], device=DEVICE)\n",
    "\n",
    "    print(f\"\\n[CLS] pos_weight={float(pos_weight.item()):.3f} (neg={n_neg}, pos={n_pos})\")\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    best_f1 = -1.0\n",
    "    best_thr = 0.5\n",
    "    best_epoch = -1\n",
    "    bad_epochs = 0\n",
    "\n",
    "    # Resume\n",
    "    resume = load_ckpt(\"cls_last.pt\")\n",
    "    start_epoch = 0\n",
    "    if resume is not None:\n",
    "        try:\n",
    "            model.load_state_dict(resume[\"model\"])\n",
    "            opt.load_state_dict(resume[\"opt\"])\n",
    "            best_f1 = float(resume.get(\"best_f1\", best_f1))\n",
    "            best_thr = float(resume.get(\"best_thr\", best_thr))\n",
    "            best_epoch = int(resume.get(\"best_epoch\", best_epoch))\n",
    "            start_epoch = int(resume[\"epoch\"]) + 1\n",
    "            bad_epochs = int(resume.get(\"bad_epochs\", 0))\n",
    "            print(f\"üîÅ [CLS] resume from epoch {start_epoch} (best_f1={best_f1:.4f})\")\n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è [CLS] could not load resume, starting fresh:\", e)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        if not budget_ok():\n",
    "            print(\"‚è±Ô∏è Budget exceeded, stopping classifier early.\")\n",
    "            break\n",
    "\n",
    "        # ---- train\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        total = 0.0\n",
    "        n = 0\n",
    "\n",
    "        print(f\"\\nüöÄ [CLS] Epoch {epoch+1}/{epochs} lr={lr:.2e}\")\n",
    "\n",
    "        for it, (x, y, _ids) in enumerate(train_loader, 1):\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE).float()\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, y, pos_weight=pos_weight)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total += float(loss.item())\n",
    "            n += 1\n",
    "\n",
    "            if it % 50 == 0:\n",
    "                print(f\"   [CLS] it {it:5d}/{len(train_loader)} loss={total/n:.4f}\")\n",
    "\n",
    "        train_loss = total / max(n,1)\n",
    "\n",
    "        # ---- val\n",
    "        model.eval()\n",
    "        probs_all = []\n",
    "        y_all = []\n",
    "        with torch.no_grad():\n",
    "            for x, y, _ids in val_loader:\n",
    "                x = x.to(DEVICE)\n",
    "                logits = model(x)\n",
    "                probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "                probs_all.append(probs)\n",
    "                y_all.append(y.numpy())\n",
    "\n",
    "        probs_all = np.concatenate(probs_all, axis=0)\n",
    "        y_all = np.concatenate(y_all, axis=0).astype(np.int32)\n",
    "\n",
    "        f1, thr, p, r = best_threshold(probs_all, y_all)\n",
    "        dt = time.time() - t0\n",
    "        print(f\"‚úÖ [CLS] epoch {epoch+1} train_loss={train_loss:.4f} val_f1={f1:.4f} thr={thr:.2f} (P={p:.3f}, R={r:.3f}) time={dt:.1f}s\")\n",
    "\n",
    "        # Save last\n",
    "        save_ckpt(\"cls_last.pt\", {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"opt\": opt.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_f1\": best_f1,\n",
    "            \"best_thr\": best_thr,\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"bad_epochs\": bad_epochs,\n",
    "        })\n",
    "\n",
    "        # Best\n",
    "        if f1 > best_f1 + 1e-5:\n",
    "            best_f1 = f1\n",
    "            best_thr = thr\n",
    "            best_epoch = epoch\n",
    "            bad_epochs = 0\n",
    "            save_ckpt(\"cls_best.pt\", {\"model\": model.state_dict(), \"best_f1\": best_f1, \"best_thr\": best_thr, \"epoch\": epoch})\n",
    "            print(f\"üíæ [CLS] New BEST saved: cls_best.pt (best_f1={best_f1:.4f}, thr={best_thr:.2f})\")\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= EARLY_STOP_PATIENCE:\n",
    "                print(f\"üõë [CLS] Early stop (no val F1 improvement for {EARLY_STOP_PATIENCE} epochs).\")\n",
    "                break\n",
    "\n",
    "    mark_done(\"CLS_DONE\", {\"epochs\": epochs, \"best_f1\": best_f1, \"best_thr\": best_thr})\n",
    "    return best_f1, best_thr\n",
    "\n",
    "# -----------------------------\n",
    "# 12) Inference + submission\n",
    "# -----------------------------\n",
    "def make_submission(model: Classifier, test_loader: DataLoader, thr: float, out_csv: Path):\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    rows = []\n",
    "    with torch.no_grad():\n",
    "        for x, pid in test_loader:\n",
    "            x = x.to(DEVICE)\n",
    "            logits = model(x)\n",
    "            probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            for pth, pr in zip(pid, probs):\n",
    "                # ID must be \"test/filename.jpg\"\n",
    "                fname = Path(pth).name\n",
    "                rows.append({\"label\": int(pr >= thr), \"ID\": f\"test/{fname}\"})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(\"‚úÖ Wrote submission:\", out_csv)\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# 13) MAIN PIPELINE\n",
    "# -----------------------------\n",
    "def main():\n",
    "    print(\"\\n[Data] quick validation samples (optional)\")\n",
    "    _good_unlab, bad_unlab = validate_images(unlab_paths, max_check=800)\n",
    "    if bad_unlab:\n",
    "        (OUT_DIR / \"bad_unlabeled_sample.json\").write_text(json.dumps(bad_unlab[:200], indent=2))\n",
    "        print(f\"‚ö†Ô∏è Found some bad unlabeled samples: {len(bad_unlab)} (logged first 200)\")\n",
    "\n",
    "    # --- Build unlabeled subset \n",
    "    ssl_paths = unlab_paths[:]\n",
    "    random.shuffle(ssl_paths)\n",
    "    if SSL_SUBSET > 0:\n",
    "        ssl_paths = ssl_paths[: min(SSL_SUBSET, len(ssl_paths))]\n",
    "        print(f\"\\n‚ö° SSL subset enabled: {len(ssl_paths):,} unlabeled images\")\n",
    "    else:\n",
    "        ssl_paths = []\n",
    "        print(\"\\n‚ö° SSL disabled (SSL_SUBSET=0)\")\n",
    "\n",
    "    print(\"\\nCounts used by this run:\")\n",
    "    print(\"Unlabeled:\", len(ssl_paths))\n",
    "    print(\"Train labeled:\", len(train_items), \" (pos=\", sum(y for _,y in train_items), \")\")\n",
    "    print(\"Val labeled  :\", len(val_items),   \" (pos=\", sum(y for _,y in val_items),   \")\")\n",
    "    print(\"Test         :\", len(test_paths))\n",
    "\n",
    "    # --- Loaders\n",
    "    train_ds = LabeledDataset(train_items, train_tf)\n",
    "    val_ds   = LabeledDataset(val_items, eval_tf)\n",
    "    test_ds  = TestDataset(test_paths)\n",
    "\n",
    "    train_loader = make_loader(train_ds, BATCH_CLS, shuffle=True)\n",
    "    val_loader   = make_loader(val_ds,   BATCH_CLS, shuffle=False)\n",
    "    test_loader  = make_loader(test_ds,  BATCH_CLS, shuffle=False)\n",
    "\n",
    "    # sanity batch sizes fixed\n",
    "    xb, yb, _ = next(iter(train_loader))\n",
    "    print(\"\\nSanity batch shapes:\", xb.shape, yb.shape, \"(must be [B,3,224,224])\")\n",
    "\n",
    "    # --- SSL stage: denoise autoencoder\n",
    "    if SSL_SUBSET > 0 and EPOCHS_SSL > 0:\n",
    "        print(\"\\n==============================\")\n",
    "        print(\"U-ONLY PIPELINE: SSL Denoise AE -> Classifier -> Submission\")\n",
    "        print(\"==============================\")\n",
    "\n",
    "        if is_done(\"SSL_DONE\") and ckpt_path(\"ssl_final.pt\").exists():\n",
    "            print(\"‚è≠Ô∏è  Skip SSL (DONE flag found).\")\n",
    "        else:\n",
    "            ssl_ds = UnlabeledDenoiseDataset(ssl_paths)\n",
    "            ssl_loader = make_loader(ssl_ds, BATCH_SSL, shuffle=True)\n",
    "\n",
    "            # Use a smaller base if memory is tight\n",
    "            ae = DenoiseAE(base=24)  # lighter than 32\n",
    "            train_ssl(ae, ssl_loader, EPOCHS_SSL, lr=2e-4)\n",
    "\n",
    "    # --- Classifier stage\n",
    "    cls = Classifier(base=24)\n",
    "\n",
    "    # If SSL exists, load encoder weights into classifier encoder\n",
    "    ssl_final = load_ckpt(\"ssl_final.pt\")\n",
    "    if ssl_final is not None and \"model\" in ssl_final:\n",
    "        print(\"\\n[Init] Loading encoder weights from SSL AE into classifier.\")\n",
    "        ae_state = ssl_final[\"model\"]\n",
    "        # Copy only encoder keys\n",
    "        enc_state = {k.replace(\"enc.\", \"\"): v for k, v in ae_state.items() if k.startswith(\"enc.\")}\n",
    "        missing, unexpected = cls.enc.load_state_dict(enc_state, strict=False)\n",
    "        print(\"Encoder load missing:\", len(missing), \"unexpected:\", len(unexpected))\n",
    "\n",
    "    best_f1, best_thr = train_classifier(cls, train_loader, val_loader, EPOCHS_CLS, lr=2e-4)\n",
    "\n",
    "    # Load BEST before submission\n",
    "    best_ckpt = load_ckpt(\"cls_best.pt\")\n",
    "    if best_ckpt is not None and \"model\" in best_ckpt:\n",
    "        cls.load_state_dict(best_ckpt[\"model\"])\n",
    "        best_thr = float(best_ckpt.get(\"best_thr\", best_thr))\n",
    "        best_f1  = float(best_ckpt.get(\"best_f1\", best_f1))\n",
    "        print(f\"\\n[Best] loaded cls_best.pt best_f1={best_f1:.4f} thr={best_thr:.2f}\")\n",
    "\n",
    "    # Submission\n",
    "    out_csv = OUT_DIR / \"submission_ucnn.csv\"\n",
    "    df = make_submission(cls, test_loader, best_thr, out_csv)\n",
    "\n",
    "    # Preview\n",
    "    print(\"\\nSubmission head:\")\n",
    "    print(df.head(10))\n",
    "\n",
    "    # Save run meta\n",
    "    meta = {\n",
    "        \"device\": str(DEVICE),\n",
    "        \"img_size\": IMG_SIZE,\n",
    "        \"ssl_subset\": SSL_SUBSET,\n",
    "        \"epochs_ssl\": EPOCHS_SSL,\n",
    "        \"epochs_cls\": EPOCHS_CLS,\n",
    "        \"best_f1\": best_f1,\n",
    "        \"best_thr\": best_thr,\n",
    "        \"time_hours\": now_h(),\n",
    "        \"train_counts\": {\n",
    "            \"train_total\": len(train_items),\n",
    "            \"train_pos\": int(sum(y for _,y in train_items)),\n",
    "            \"train_neg\": int(sum(1-y for _,y in train_items)),\n",
    "            \"val_total\": len(val_items),\n",
    "            \"val_pos\": int(sum(y for _,y in val_items)),\n",
    "            \"val_neg\": int(sum(1-y for _,y in val_items)),\n",
    "        },\n",
    "    }\n",
    "    save_json(OUT_DIR / \"run_meta.json\", meta)\n",
    "    print(\"\\n‚úÖ Wrote meta:\", OUT_DIR / \"run_meta.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
