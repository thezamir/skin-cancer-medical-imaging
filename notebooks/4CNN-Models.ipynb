{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WYj0T_qZ3-E"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 4 Models Script\n",
        "# ResNet18 / ResNet34 / DenseNet121 / ConvNeXt-Tiny (NO pretrained weights)\n",
        "# + Oversampling + Best F1 model (argmax for selection)\n",
        "# + Choose BEST threshold on VAL (maximize F1)\n",
        "# + Produce 4 Kaggle submissions (one per model)\n",
        "# BEST SUBMISSION was with LR = 5e-4 and 50 epochs, weight_decay=1e-4 val_pos/neg = int(0.15 * count)\n",
        "# =========================\n",
        "\n",
        "!pip -q install scikit-learn pandas tqdm matplotlib seaborn\n",
        "\n",
        "import os, re, random, shutil, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler, Subset\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# -------------------------\n",
        "# Mount Drive (safe)\n",
        "# -------------------------\n",
        "from google.colab import drive\n",
        "if not os.path.exists(\"/content/drive\"):\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "SEED = 42\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 50\n",
        "LR = 2e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "NUM_WORKERS = 2\n",
        "IMG_SIZE = 224\n",
        "\n",
        "VAL_FRAC_POS = 0.15\n",
        "VAL_FRAC_NEG = 0.15\n",
        "\n",
        "ROOT = \"/content/drive/MyDrive/Colab Notebooks/Course/Skin_Cancer_Classification\"\n",
        "\n",
        "# expected structure:\n",
        "# ROOT/train/0 , ROOT/train/1 , ROOT/test\n",
        "TRAIN_ROOT = os.path.join(ROOT, \"train\")\n",
        "TEST_DIR   = os.path.join(ROOT, \"test\")\n",
        "\n",
        "MASTER_OUT_DIR = os.path.join(ROOT, \"models_multi_backbones_scratch\")\n",
        "os.makedirs(MASTER_OUT_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Reproducibility\n",
        "# -------------------------\n",
        "def seed_everything(seed=SEED):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# -------------------------\n",
        "# Step 1: Ensure correct folder structure (move ROOT/0 and ROOT/1 into ROOT/train/0, ROOT/train/1 if needed)\n",
        "# -------------------------\n",
        "os.makedirs(TRAIN_ROOT, exist_ok=True)\n",
        "\n",
        "for cls in [\"0\", \"1\"]:\n",
        "    src_root = os.path.join(ROOT, cls)\n",
        "    dst_train = os.path.join(TRAIN_ROOT, cls)\n",
        "\n",
        "    if os.path.exists(dst_train):\n",
        "        print(f\"[OK] {dst_train} exists\")\n",
        "        continue\n",
        "\n",
        "    if os.path.exists(src_root):\n",
        "        print(f\"[MOVE] {src_root}  -->  {dst_train}\")\n",
        "        shutil.move(src_root, dst_train)\n",
        "    else:\n",
        "        print(f\"[WARN] Missing: {src_root} (and {dst_train} not found)\")\n",
        "\n",
        "assert os.path.isdir(os.path.join(TRAIN_ROOT, \"0\")), \"train/0 not found\"\n",
        "assert os.path.isdir(os.path.join(TRAIN_ROOT, \"1\")), \"train/1 not found\"\n",
        "assert os.path.isdir(TEST_DIR), \"test folder not found\"\n",
        "\n",
        "# -------------------------\n",
        "# Transforms\n",
        "# -------------------------\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.85, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(p=0.2),\n",
        "    transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.10, hue=0.02),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "val_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "# -------------------------\n",
        "# Step 2: Dataset + stratified train/val split (SAME split shared by all models)\n",
        "# -------------------------\n",
        "full_ds_train = datasets.ImageFolder(root=TRAIN_ROOT, transform=train_tfms)\n",
        "print(\"Classes:\", full_ds_train.classes, \"class_to_idx:\", full_ds_train.class_to_idx)\n",
        "\n",
        "targets = np.array([y for _, y in full_ds_train.samples])\n",
        "idx_all = np.arange(len(full_ds_train))\n",
        "\n",
        "pos_idx = idx_all[targets == 1]\n",
        "neg_idx = idx_all[targets == 0]\n",
        "\n",
        "rng = np.random.default_rng(SEED)\n",
        "rng.shuffle(pos_idx)\n",
        "rng.shuffle(neg_idx)\n",
        "\n",
        "val_pos = int(VAL_FRAC_POS * len(pos_idx))\n",
        "val_neg = int(VAL_FRAC_NEG * len(neg_idx))\n",
        "\n",
        "val_idx = np.concatenate([pos_idx[:val_pos], neg_idx[:val_neg]])\n",
        "train_idx = np.concatenate([pos_idx[val_pos:], neg_idx[val_neg:]])\n",
        "rng.shuffle(train_idx)\n",
        "rng.shuffle(val_idx)\n",
        "\n",
        "train_ds = Subset(full_ds_train, train_idx)\n",
        "\n",
        "full_ds_val = datasets.ImageFolder(root=TRAIN_ROOT, transform=val_tfms)\n",
        "val_ds = Subset(full_ds_val, val_idx)\n",
        "\n",
        "train_targets = targets[train_idx]\n",
        "n0 = int((train_targets == 0).sum())\n",
        "n1 = int((train_targets == 1).sum())\n",
        "print(f\"Train count: 0={n0}, 1={n1} | ratio0/1={n0/max(n1,1):.2f}\")\n",
        "\n",
        "val_targets = targets[val_idx]\n",
        "vn0 = int((val_targets == 0).sum())\n",
        "vn1 = int((val_targets == 1).sum())\n",
        "print(f\"VAL count:   0={vn0}, 1={vn1} | ratio0/1={vn0/max(vn1,1):.2f}\")\n",
        "\n",
        "# -------------------------\n",
        "# Test dataset helper\n",
        "# -------------------------\n",
        "test_files = [f for f in os.listdir(TEST_DIR) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
        "assert len(test_files) > 0, \"No images found in test folder\"\n",
        "\n",
        "def extract_number(fn: str) -> int:\n",
        "    m = re.findall(r\"\\d+\", fn)\n",
        "    return int(m[-1]) if m else -1\n",
        "\n",
        "test_files = sorted(test_files, key=extract_number)\n",
        "\n",
        "def filename_to_kaggle_id(fn: str) -> str:\n",
        "    num = extract_number(fn)\n",
        "    return f\"test/{num:06d}.jpg\"\n",
        "\n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder, files, transform):\n",
        "        self.folder = folder\n",
        "        self.files = files\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        from PIL import Image\n",
        "        fn = self.files[idx]\n",
        "        path = os.path.join(self.folder, fn)\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        return img, fn\n",
        "\n",
        "test_ds = TestDataset(TEST_DIR, test_files, val_tfms)\n",
        "test_loader = DataLoader(\n",
        "    test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=True\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Model factory (all from scratch)\n",
        "# -------------------------\n",
        "def build_model(name: str) -> nn.Module:\n",
        "    name = name.lower().strip()\n",
        "\n",
        "    if name == \"resnet18\":\n",
        "        m = models.resnet18(weights=None)\n",
        "        m.fc = nn.Linear(m.fc.in_features, 2)\n",
        "        return m\n",
        "\n",
        "    if name == \"resnet34\":\n",
        "        m = models.resnet34(weights=None)\n",
        "        m.fc = nn.Linear(m.fc.in_features, 2)\n",
        "        return m\n",
        "\n",
        "    if name == \"densenet121\":\n",
        "        m = models.densenet121(weights=None)\n",
        "        m.classifier = nn.Linear(m.classifier.in_features, 2)\n",
        "        return m\n",
        "\n",
        "    if name in [\"convnext_tiny\", \"convnext-tiny\", \"convnexttiny\"]:\n",
        "        m = models.convnext_tiny(weights=None)\n",
        "        # torchvision convnext has classifier = Sequential(..., Linear)\n",
        "        last_linear = m.classifier[-1]\n",
        "        assert isinstance(last_linear, nn.Linear)\n",
        "        m.classifier[-1] = nn.Linear(last_linear.in_features, 2)\n",
        "        return m\n",
        "\n",
        "    raise ValueError(f\"Unknown model name: {name}\")\n",
        "\n",
        "# -------------------------\n",
        "# Metrics helpers\n",
        "# -------------------------\n",
        "@torch.no_grad()\n",
        "def eval_f1_argmax(model, loader):\n",
        "    model.eval()\n",
        "    ys, preds = [], []\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
        "            logits = model(x)\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "        ys.append(y.detach().cpu().numpy())\n",
        "        preds.append(pred.detach().cpu().numpy())\n",
        "    ys = np.concatenate(ys)\n",
        "    preds = np.concatenate(preds)\n",
        "    return float(f1_score(ys, preds, pos_label=1))\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_val_probs(model, loader):\n",
        "    model.eval()\n",
        "    probs, ys = [], []\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
        "            logits = model(x)\n",
        "            p1 = torch.softmax(logits, dim=1)[:, 1]\n",
        "        probs.append(p1.detach().cpu().numpy())\n",
        "        ys.append(y.detach().cpu().numpy())\n",
        "    return np.concatenate(probs), np.concatenate(ys)\n",
        "\n",
        "def best_threshold_max_f1(p_val, y_val):\n",
        "    ths = np.linspace(0.05, 0.99, 95)\n",
        "    best = {\"th\": None, \"f1\": -1.0, \"prec\": None, \"rec\": None}\n",
        "    for th in ths:\n",
        "        pred = (p_val > th).astype(int)\n",
        "        f1 = f1_score(y_val, pred, pos_label=1)\n",
        "        if f1 > best[\"f1\"]:\n",
        "            best[\"f1\"] = float(f1)\n",
        "            best[\"th\"] = float(th)\n",
        "            best[\"prec\"] = float(precision_score(y_val, pred, pos_label=1, zero_division=0))\n",
        "            best[\"rec\"]  = float(recall_score(y_val, pred, pos_label=1, zero_division=0))\n",
        "    return best\n",
        "\n",
        "# -------------------------\n",
        "# Train + submit one model\n",
        "# -------------------------\n",
        "def run_one_model(model_name: str):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"RUN MODEL: {model_name}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    out_dir = os.path.join(MASTER_OUT_DIR, model_name)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    best_model_path = os.path.join(out_dir, f\"best_{model_name}_f1.pt\")\n",
        "    sub_path = os.path.join(out_dir, f\"submission_{model_name}_bestValThreshold.csv\")\n",
        "\n",
        "    # Oversampling via WeightedRandomSampler on TRAIN only\n",
        "    class_counts = np.bincount(train_targets, minlength=2).astype(np.float64)\n",
        "    class_weights = 1.0 / np.maximum(class_counts, 1.0)\n",
        "    sample_weights = class_weights[train_targets]\n",
        "\n",
        "    sampler = WeightedRandomSampler(\n",
        "        weights=torch.from_numpy(sample_weights).double(),\n",
        "        num_samples=len(train_targets),\n",
        "        replacement=True\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, sampler=sampler,\n",
        "        num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=NUM_WORKERS, pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Build model\n",
        "    model = build_model(model_name).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
        "\n",
        "    # Train (save best by val F1 using argmax for selection)\n",
        "    best_f1 = -1.0\n",
        "    best_epoch = -1\n",
        "\n",
        "    epoch_train_losses = []\n",
        "    epoch_train_accuracies = []\n",
        "    epoch_val_f1s = []\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"{model_name} | Epoch {epoch}/{EPOCHS}\", leave=False)\n",
        "        for x, y in pbar:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            y = y.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits, y)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item() * x.size(0)\n",
        "            pbar.set_postfix(loss=float(loss.item()))\n",
        "\n",
        "            _, predicted = torch.max(logits.data, 1)\n",
        "            total_samples += y.size(0)\n",
        "            correct_predictions += (predicted == y).sum().item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_ds)\n",
        "        train_accuracy = correct_predictions / total_samples\n",
        "        val_f1 = eval_f1_argmax(model, val_loader)\n",
        "\n",
        "        epoch_train_losses.append(avg_loss)\n",
        "        epoch_train_accuracies.append(train_accuracy)\n",
        "        epoch_val_f1s.append(val_f1)\n",
        "\n",
        "        print(f\"{model_name} | Epoch {epoch:02d} | train_loss={avg_loss:.4f} | train_acc={train_accuracy:.4f} | val_f1(argmax)={val_f1:.4f}\")\n",
        "\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1 = val_f1\n",
        "            best_epoch = epoch\n",
        "            torch.save(\n",
        "                {\"model_state\": model.state_dict(), \"best_f1\": best_f1, \"epoch\": best_epoch,\n",
        "                 \"train_losses_history\": epoch_train_losses, \"train_accuracies_history\": epoch_train_accuracies, \"val_f1s_history\": epoch_val_f1s},\n",
        "                best_model_path\n",
        "            )\n",
        "            print(f\"  -> saved BEST model (val_f1={best_f1:.4f})\")\n",
        "\n",
        "    print(f\"{model_name} | Training done. Best val F1(argmax)={best_f1:.4f} at epoch {best_epoch}\")\n",
        "    print(\"Best model saved to:\", best_model_path)\n",
        "\n",
        "    # Load best to ensure we have the full history from the best run\n",
        "    ckpt = torch.load(best_model_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "    model.eval()\n",
        "\n",
        "    # Choose best threshold on VAL\n",
        "    p_val, y_val = collect_val_probs(model, val_loader)\n",
        "    best = best_threshold_max_f1(p_val, y_val)\n",
        "    P_THRESH = best[\"th\"]\n",
        "\n",
        "    print(f\"{model_name} | Best threshold on VAL (maximize F1): {best}\")\n",
        "    print(f\"{model_name} | Using threshold: {P_THRESH}\")\n",
        "\n",
        "    # Predict test + save submission\n",
        "    all_ids, all_p = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, fns in tqdm(test_loader, desc=f\"{model_name} | Predict test\"):\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
        "                logits = model(x)\n",
        "                p1 = torch.softmax(logits, dim=1)[:, 1]\n",
        "            p1 = p1.detach().cpu().numpy()\n",
        "\n",
        "            for fn, prob1 in zip(fns, p1):\n",
        "                all_ids.append(filename_to_kaggle_id(fn))\n",
        "                all_p.append(float(prob1))\n",
        "\n",
        "    labels = (np.array(all_p) > P_THRESH).astype(int)\n",
        "    sub = pd.DataFrame({\"ID\": all_ids, \"label\": labels})\n",
        "    sub.to_csv(sub_path, index=False)\n",
        "\n",
        "    print(f\"{model_name} | Saved submission: {sub_path}\")\n",
        "    print(sub.head())\n",
        "    print(f\"{model_name} | Predicted positives: {int(sub['label'].sum())} / {len(sub)}\")\n",
        "\n",
        "    # Cleanup\n",
        "    del model, optimizer, scaler, train_loader, val_loader\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return {\n",
        "        \"model\": model_name,\n",
        "        \"best_val_f1_argmax\": float(ckpt.get(\"best_f1\", -1.0)),\n",
        "        \"best_epoch\": int(ckpt.get(\"epoch\", -1)),\n",
        "        \"best_thresh\": float(P_THRESH),\n",
        "        \"val_thresh_f1\": float(best[\"f1\"]),\n",
        "        \"val_thresh_prec\": float(best[\"prec\"]),\n",
        "        \"val_thresh_rec\": float(best[\"rec\"]),\n",
        "        \"submission_path\": sub_path,\n",
        "        \"best_model_path\": best_model_path,\n",
        "        \"train_losses_history\": ckpt.get(\"train_losses_history\", []),\n",
        "        \"train_accuracies_history\": ckpt.get(\"train_accuracies_history\", []),\n",
        "        \"val_f1s_history\": ckpt.get(\"val_f1s_history\", []),\n",
        "    }\n",
        "\n",
        "# -------------------------\n",
        "# Run 4 models\n",
        "# -------------------------\n",
        "MODELS_TO_RUN = [\"resnet18\", \"resnet34\", \"densenet121\", \"convnext_tiny\"]\n",
        "\n",
        "results = []\n",
        "for mname in MODELS_TO_RUN:\n",
        "    results.append(run_one_model(mname))\n",
        "\n",
        "# Summary table\n",
        "res_df = pd.DataFrame(results).sort_values(by=\"val_thresh_f1\", ascending=False)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY (sorted by VAL F1 using best threshold)\")\n",
        "print(\"=\"*80)\n",
        "display(res_df)\n",
        "\n",
        "# Prepare data for plotting\n",
        "plot_data_acc = []\n",
        "plot_data_f1 = []\n",
        "\n",
        "for res in results:\n",
        "    model_name = res['model']\n",
        "    train_accuracies = res['train_accuracies_history']\n",
        "    val_f1s = res['val_f1s_history']\n",
        "\n",
        "    for epoch, acc in enumerate(train_accuracies):\n",
        "        plot_data_acc.append({'model': model_name, 'epoch': epoch + 1, 'metric': 'Training Accuracy', 'value': acc})\n",
        "    for epoch, f1 in enumerate(val_f1s):\n",
        "        plot_data_f1.append({'model': model_name, 'epoch': epoch + 1, 'metric': 'Validation F1', 'value': f1})\n",
        "\n",
        "df_plot_acc = pd.DataFrame(plot_data_acc)\n",
        "df_plot_f1 = pd.DataFrame(plot_data_f1)\n",
        "\n",
        "# Plot Training Accuracy\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=df_plot_acc, x='epoch', y='value', hue='model', marker='o')\n",
        "plt.title('Training Accuracy vs. Epoch for All Models')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Accuracy')\n",
        "plt.grid(True)\n",
        "plt.legend(title='Model')\n",
        "plt.show()\n",
        "\n",
        "# Plot Validation F1 Score\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=df_plot_f1, x='epoch', y='value', hue='model', marker='o')\n",
        "plt.title('Validation F1 Score vs. Epoch for All Models')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation F1 Score (argmax)')\n",
        "plt.grid(True)\n",
        "plt.legend(title='Model')\n",
        "plt.show()"
      ]
    }
  ]
}